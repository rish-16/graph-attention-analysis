{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f527d905-8b41-4c62-940c-37cba7b9a06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric as tg\n",
    "import torch_geometric.nn as tgnn\n",
    "from torch_geometric.utils import get_laplacian, to_dense_adj\n",
    "from torch_geometric.typing import Adj, OptTensor, PairTensor\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, Tuple, Union\n",
    "import math, wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4abf4a7a-d5db-4975-8041-72195c642a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Eigen(nn.Module):\n",
    "    def __init__(self, k):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "    \n",
    "    def forward(self, edge_idx):\n",
    "        lap_idx, lap_wt = get_laplacian(edge_idx, normalization=\"sym\")\n",
    "        lap_adj = to_dense_adj(lap_idx)\n",
    "        eigenvals, eigenvecs = torch.linalg.eig(lap_adj)\n",
    "        top_eig = eigenvecs.squeeze(0)[:, 1:self.k+1]\n",
    "        top_eig = torch.real(top_eig)\n",
    "        new_edge_features = torch.Tensor(edge_idx.size(1), 2 * self.k)\n",
    "        new_edge_idx = edge_idx.T\n",
    "\n",
    "        for idx, pair in enumerate(new_edge_idx):\n",
    "            i, j = pair\n",
    "            x_i_prime = top_eig[i]\n",
    "            x_j_prime = top_eig[j]\n",
    "            new_feat = torch.cat([x_i_prime, x_j_prime], dim=0)\n",
    "            new_edge_features[idx] = new_feat\n",
    "\n",
    "        return new_edge_features\n",
    "\n",
    "class GATv3Layer(tgnn.MessagePassing):\n",
    "    def __init__(self, indim, eigendim, outdim):\n",
    "        super().__init__(aggr=\"add\")\n",
    "        self.original_mlp = nn.Sequential(\n",
    "                nn.Linear(2 * indim, outdim), # account for extra Wx_i || Wx_j from GATv1\n",
    "                nn.Linear(outdim, outdim),\n",
    "                nn.LeakyReLU(0.02),\n",
    "                nn.Linear(outdim, outdim)\n",
    "            )\n",
    "        \n",
    "        self.eigen_mlp = nn.Sequential(\n",
    "                nn.Linear(eigendim, outdim), # account for the fact that edge attributes are already concatenated\n",
    "                nn.Linear(outdim, outdim),\n",
    "                nn.LeakyReLU(0.02),\n",
    "                nn.Linear(outdim, outdim)\n",
    "            )\n",
    "        self.W = nn.Linear(indim, indim)\n",
    "        self.project = nn.Linear(outdim, 1)\n",
    "        self.out = nn.Linear(indim, outdim)\n",
    "\n",
    "        self.alpha = nn.Parameter(torch.rand(1, 1))\n",
    "        self.glorot(self.alpha)\n",
    "\n",
    "        self.beta = nn.Parameter(torch.rand(1, 1))\n",
    "        self.glorot(self.beta)\n",
    "        \n",
    "        self.all_gammas = None\n",
    "        \n",
    "    def glorot(self, value):\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            stdv = math.sqrt(6.0 / (value.size(-2) + value.size(-1)))\n",
    "            value.data.uniform_(-stdv, stdv)\n",
    "        else:\n",
    "            for v in value.parameters() if hasattr(value, 'parameters') else []:\n",
    "                glorot(v)\n",
    "            for v in value.buffers() if hasattr(value, 'buffers') else []:\n",
    "                glorot(v)\n",
    "        \n",
    "    def forward(self, x, edge_idx, edge_attr):\n",
    "        num_nodes = x.size(0)\n",
    "        edge_idx, edge_attr = tg.utils.remove_self_loops(edge_idx, edge_attr)\n",
    "        edge_idx, edge_attr = tg.utils.add_self_loops(edge_idx, edge_attr, num_nodes=num_nodes)\n",
    "        \n",
    "        return self.propagate(edge_idx, x=x, edge_attr=edge_attr)\n",
    "\n",
    "    def message(self, x_j: torch.Tensor, x_i: torch.Tensor,\n",
    "                edge_attr: torch.Tensor,\n",
    "                index: torch.Tensor, ptr: OptTensor,\n",
    "                size_i: Optional[int]) -> torch.Tensor:\n",
    "    \n",
    "        cat = torch.cat([x_i, x_j], dim=1)\n",
    "        \n",
    "        node_attr = self.alpha * self.original_mlp(cat) # [E, d]\n",
    "        edge_attr = self.beta * self.eigen_mlp(edge_attr) # [E, d]\n",
    "        \n",
    "        temp = F.leaky_relu(node_attr + edge_attr) # [E, d]\n",
    "        project = self.project(temp)\n",
    "        gamma = tg.utils.softmax(project, index, ptr, size_i) # [E, d]\n",
    "        msg = gamma * self.out(x_j) # [E, d]\n",
    "        \n",
    "        self.all_gammas = gamma\n",
    "        \n",
    "        return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "75c93e24-02f7-450e-bb08-4617470d5fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATv3(nn.Module):\n",
    "    def __init__(self, indim, eigendim, hidden, outdim, k):\n",
    "        super().__init__()\n",
    "\n",
    "        self.eigen = Eigen(k)\n",
    "        self.gat1 = GATv3Layer(indim, eigendim, hidden)\n",
    "        self.gat2 = GATv3Layer(hidden, eigendim, outdim)\n",
    "\n",
    "    def forward(self, x, edge_idx):\n",
    "        with torch.no_grad():\n",
    "            eigen_x = self.eigen(edge_idx)\n",
    "        x = torch.relu(self.gat1(x, edge_idx, eigen_x))\n",
    "        out = self.gat2(x, edge_idx, eigen_x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "283433f7-4d02-471a-881a-409e721b2c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gammas(Xw, ground_truth, gat_layer, edge_idx):\n",
    "    all_gammas = gat_layer.all_gammas\n",
    "    gamma_matrix = [[0 for j in range(Xw.size(0))] for i in range(Xw.size(0))]\n",
    "    for idx, pair in enumerate(edge_idx.T):\n",
    "        i, j = pair\n",
    "        gamma = all_gammas[idx]\n",
    "        \n",
    "        gamma_matrix[i][j] = gamma.item()\n",
    "        \n",
    "    return gamma_matrix\n",
    "        \n",
    "def get_intra_inter_avg_gamma(gamma_matrix):\n",
    "    \"\"\"\n",
    "    intra-edges are nodes with class 0\n",
    "    inter-edges are nodes with class 1\n",
    "    \"\"\"\n",
    "    \n",
    "    d = len(gamma_matrix) // 2\n",
    "    all_node_ids = list(range(len(gamma_matrix)))\n",
    "    intra_edges = all_node_ids[:d]\n",
    "    inter_edges = all_node_ids[d:]\n",
    "    \n",
    "    intra_edge_gammas = []\n",
    "    inter_edge_gammas = []\n",
    "    \n",
    "    for i in range(len(gamma_matrix)):\n",
    "        for j in range(len(gamma_matrix[i])):\n",
    "            if j in intra_edges:\n",
    "                intra_edge_gammas.append(gamma_matrix[i][j])\n",
    "            elif j in inter_edges:\n",
    "                inter_edge_gammas.append(gamma_matrix[i][j])\n",
    "            else:\n",
    "                pass\n",
    "                \n",
    "    return np.array(intra_edge_gammas), np.array(inter_edge_gammas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7ba72acd-2f8c-45b9-995e-b5f2d7a89a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3fyhzoul) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_bce</td><td>▂▂▂▂▂▂▂▂▂▂▂▂▃▃▃▃▂▂▂▂▃▃▄▅▆▆▅▄▃▂▁▁▁▂▃▄▅▆██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>61</td></tr><tr><td>train_bce</td><td>0.69424</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">dandy-serenity-10</strong>: <a href=\"https://wandb.ai/rish-16/GATv3/runs/3fyhzoul\" target=\"_blank\">https://wandb.ai/rish-16/GATv3/runs/3fyhzoul</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220525_002128-3fyhzoul/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3fyhzoul). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/rishabh/Desktop/graph-attention-analysis/src/wandb/run-20220525_002204-3b873ryl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/rish-16/GATv3/runs/3b873ryl\" target=\"_blank\">fearless-dawn-11</a></strong> to <a href=\"https://wandb.ai/rish-16/GATv3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Mu: 0.00014433756729740645\n",
      "Epoch: 0 | Train BCE: 0.7008239030838013\n",
      "Epoch: 200 | Train BCE: 0.5811325311660767\n",
      "Epoch: 400 | Train BCE: 0.4728632867336273\n",
      "Epoch: 600 | Train BCE: 0.3233521580696106\n",
      "Epoch: 800 | Train BCE: 0.12481539696455002\n",
      "Epoch: 1000 | Train BCE: 0.04695728421211243\n",
      "Epoch: 1200 | Train BCE: 0.011814644560217857\n",
      "------------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_inter_edge_gamma_1</td><td>▁</td></tr><tr><td>avg_inter_edge_gamma_2</td><td>▁</td></tr><tr><td>avg_intra_edge_gamma_1</td><td>▁</td></tr><tr><td>avg_intra_edge_gamma_2</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>mu</td><td>▁</td></tr><tr><td>std_inter_edge_gamma_1</td><td>▁</td></tr><tr><td>std_inter_edge_gamma_2</td><td>▁</td></tr><tr><td>std_intra_edge_gamma_1</td><td>▁</td></tr><tr><td>std_intra_edge_gamma_2</td><td>▁</td></tr><tr><td>train_bce</td><td>█████▇▇▇▇▆▅▅▆▅▇▄▅▃▄▄▃▃▄▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_inter_edge_gamma_1</td><td>0.0025</td></tr><tr><td>avg_inter_edge_gamma_2</td><td>0.00234</td></tr><tr><td>avg_intra_edge_gamma_1</td><td>0.0025</td></tr><tr><td>avg_intra_edge_gamma_2</td><td>0.00234</td></tr><tr><td>epoch</td><td>1249</td></tr><tr><td>mu</td><td>0.00014</td></tr><tr><td>std_inter_edge_gamma_1</td><td>0.04994</td></tr><tr><td>std_inter_edge_gamma_2</td><td>0.00358</td></tr><tr><td>std_intra_edge_gamma_1</td><td>0.04994</td></tr><tr><td>std_intra_edge_gamma_2</td><td>0.00358</td></tr><tr><td>train_bce</td><td>0.00953</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">fearless-dawn-11</strong>: <a href=\"https://wandb.ai/rish-16/GATv3/runs/3b873ryl\" target=\"_blank\">https://wandb.ai/rish-16/GATv3/runs/3b873ryl</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220525_002204-3b873ryl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/rishabh/Desktop/graph-attention-analysis/src/wandb/run-20220525_004151-whk0opbb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/rish-16/GATv3/runs/whk0opbb\" target=\"_blank\">atomic-donkey-12</a></strong> to <a href=\"https://wandb.ai/rish-16/GATv3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Mu: 0.00019579604521621774\n",
      "Epoch: 0 | Train BCE: 0.693331778049469\n",
      "Epoch: 200 | Train BCE: 0.6030031442642212\n",
      "Epoch: 400 | Train BCE: 0.42886388301849365\n",
      "Epoch: 600 | Train BCE: 0.4737018346786499\n",
      "Epoch: 800 | Train BCE: 0.11750677227973938\n",
      "Epoch: 1000 | Train BCE: 0.29766231775283813\n",
      "Epoch: 1200 | Train BCE: 0.09280756115913391\n",
      "------------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_inter_edge_gamma_1</td><td>▁</td></tr><tr><td>avg_inter_edge_gamma_2</td><td>▁</td></tr><tr><td>avg_intra_edge_gamma_1</td><td>▁</td></tr><tr><td>avg_intra_edge_gamma_2</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>mu</td><td>▁</td></tr><tr><td>std_inter_edge_gamma_1</td><td>▁</td></tr><tr><td>std_inter_edge_gamma_2</td><td>▁</td></tr><tr><td>std_intra_edge_gamma_1</td><td>▁</td></tr><tr><td>std_intra_edge_gamma_2</td><td>▁</td></tr><tr><td>train_bce</td><td>█████▇▇▇▆▆▆▆▅▅▅▅▅▅▅▅▅▃▃▃▃▂▅▃▅▂▂▄▃▂▁▁▃▂▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_inter_edge_gamma_1</td><td>0.0025</td></tr><tr><td>avg_inter_edge_gamma_2</td><td>0.0</td></tr><tr><td>avg_intra_edge_gamma_1</td><td>0.0025</td></tr><tr><td>avg_intra_edge_gamma_2</td><td>0.0</td></tr><tr><td>epoch</td><td>1249</td></tr><tr><td>mu</td><td>0.0002</td></tr><tr><td>std_inter_edge_gamma_1</td><td>0.04994</td></tr><tr><td>std_inter_edge_gamma_2</td><td>0.0</td></tr><tr><td>std_intra_edge_gamma_1</td><td>0.04994</td></tr><tr><td>std_intra_edge_gamma_2</td><td>0.0</td></tr><tr><td>train_bce</td><td>0.18855</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">atomic-donkey-12</strong>: <a href=\"https://wandb.ai/rish-16/GATv3/runs/whk0opbb\" target=\"_blank\">https://wandb.ai/rish-16/GATv3/runs/whk0opbb</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220525_004151-whk0opbb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/rishabh/Desktop/graph-attention-analysis/src/wandb/run-20220525_010139-1j20rs8e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/rish-16/GATv3/runs/1j20rs8e\" target=\"_blank\">fallen-music-13</a></strong> to <a href=\"https://wandb.ai/rish-16/GATv3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Mu: 0.00026560023173537287\n",
      "Epoch: 0 | Train BCE: 0.7005781531333923\n",
      "Epoch: 200 | Train BCE: 0.6709121465682983\n",
      "Epoch: 400 | Train BCE: 0.39175766706466675\n",
      "Epoch: 600 | Train BCE: 0.23879766464233398\n",
      "Epoch: 800 | Train BCE: 0.05040017515420914\n",
      "Epoch: 1000 | Train BCE: 0.030592476949095726\n",
      "Epoch: 1200 | Train BCE: 0.02673676237463951\n",
      "------------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_inter_edge_gamma_1</td><td>▁</td></tr><tr><td>avg_inter_edge_gamma_2</td><td>▁</td></tr><tr><td>avg_intra_edge_gamma_1</td><td>▁</td></tr><tr><td>avg_intra_edge_gamma_2</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>mu</td><td>▁</td></tr><tr><td>std_inter_edge_gamma_1</td><td>▁</td></tr><tr><td>std_inter_edge_gamma_2</td><td>▁</td></tr><tr><td>std_intra_edge_gamma_1</td><td>▁</td></tr><tr><td>std_intra_edge_gamma_2</td><td>▁</td></tr><tr><td>train_bce</td><td>███████▇▇▆▅▄▃▃▅▃▄▃▃▂▃▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_inter_edge_gamma_1</td><td>0.0025</td></tr><tr><td>avg_inter_edge_gamma_2</td><td>0.00248</td></tr><tr><td>avg_intra_edge_gamma_1</td><td>0.0025</td></tr><tr><td>avg_intra_edge_gamma_2</td><td>0.00248</td></tr><tr><td>epoch</td><td>1249</td></tr><tr><td>mu</td><td>0.00027</td></tr><tr><td>std_inter_edge_gamma_1</td><td>0.04994</td></tr><tr><td>std_inter_edge_gamma_2</td><td>0.00381</td></tr><tr><td>std_intra_edge_gamma_1</td><td>0.04994</td></tr><tr><td>std_intra_edge_gamma_2</td><td>0.0038</td></tr><tr><td>train_bce</td><td>0.01264</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">fallen-music-13</strong>: <a href=\"https://wandb.ai/rish-16/GATv3/runs/1j20rs8e\" target=\"_blank\">https://wandb.ai/rish-16/GATv3/runs/1j20rs8e</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220525_010139-1j20rs8e/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/rishabh/Desktop/graph-attention-analysis/src/wandb/run-20220525_012113-17aqlbaz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/rish-16/GATv3/runs/17aqlbaz\" target=\"_blank\">quiet-terrain-14</a></strong> to <a href=\"https://wandb.ai/rish-16/GATv3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Mu: 0.00036029064335790133\n",
      "Epoch: 0 | Train BCE: 0.7025562524795532\n",
      "Epoch: 200 | Train BCE: 0.639792263507843\n",
      "Epoch: 400 | Train BCE: 0.47997409105300903\n",
      "Epoch: 600 | Train BCE: 0.11061519384384155\n",
      "Epoch: 800 | Train BCE: 0.393920361995697\n",
      "Epoch: 1000 | Train BCE: 0.17294567823410034\n",
      "Epoch: 1200 | Train BCE: 0.289513498544693\n",
      "------------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_inter_edge_gamma_1</td><td>▁</td></tr><tr><td>avg_inter_edge_gamma_2</td><td>▁</td></tr><tr><td>avg_intra_edge_gamma_1</td><td>▁</td></tr><tr><td>avg_intra_edge_gamma_2</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>mu</td><td>▁</td></tr><tr><td>std_inter_edge_gamma_1</td><td>▁</td></tr><tr><td>std_inter_edge_gamma_2</td><td>▁</td></tr><tr><td>std_intra_edge_gamma_1</td><td>▁</td></tr><tr><td>std_intra_edge_gamma_2</td><td>▁</td></tr><tr><td>train_bce</td><td>██████▇▇▇▆▆▄▅▆▆▅▅▄▂▂▁▃▁█▄▅▃▂▂▁▁▁▁▁▄▂▅▄▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_inter_edge_gamma_1</td><td>0.0025</td></tr><tr><td>avg_inter_edge_gamma_2</td><td>0.0</td></tr><tr><td>avg_intra_edge_gamma_1</td><td>0.0025</td></tr><tr><td>avg_intra_edge_gamma_2</td><td>0.0</td></tr><tr><td>epoch</td><td>1249</td></tr><tr><td>mu</td><td>0.00036</td></tr><tr><td>std_inter_edge_gamma_1</td><td>0.04887</td></tr><tr><td>std_inter_edge_gamma_2</td><td>0.0</td></tr><tr><td>std_intra_edge_gamma_1</td><td>0.04994</td></tr><tr><td>std_intra_edge_gamma_2</td><td>0.0</td></tr><tr><td>train_bce</td><td>0.10525</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">quiet-terrain-14</strong>: <a href=\"https://wandb.ai/rish-16/GATv3/runs/17aqlbaz\" target=\"_blank\">https://wandb.ai/rish-16/GATv3/runs/17aqlbaz</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220525_012113-17aqlbaz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/rishabh/Desktop/graph-attention-analysis/src/wandb/run-20220525_014040-3itep3wk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/rish-16/GATv3/runs/3itep3wk\" target=\"_blank\">icy-glitter-15</a></strong> to <a href=\"https://wandb.ai/rish-16/GATv3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Mu: 0.0004887395874736438\n",
      "Epoch: 0 | Train BCE: 0.6932708024978638\n",
      "Epoch: 200 | Train BCE: 0.6949211359024048\n",
      "Epoch: 400 | Train BCE: 0.46762844920158386\n",
      "Epoch: 600 | Train BCE: 0.5078461170196533\n",
      "Epoch: 800 | Train BCE: 0.46960216760635376\n",
      "Epoch: 1000 | Train BCE: 0.28918030858039856\n",
      "Epoch: 1200 | Train BCE: 0.2831963896751404\n",
      "------------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_inter_edge_gamma_1</td><td>▁</td></tr><tr><td>avg_inter_edge_gamma_2</td><td>▁</td></tr><tr><td>avg_intra_edge_gamma_1</td><td>▁</td></tr><tr><td>avg_intra_edge_gamma_2</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>mu</td><td>▁</td></tr><tr><td>std_inter_edge_gamma_1</td><td>▁</td></tr><tr><td>std_inter_edge_gamma_2</td><td>▁</td></tr><tr><td>std_intra_edge_gamma_1</td><td>▁</td></tr><tr><td>std_intra_edge_gamma_2</td><td>▁</td></tr><tr><td>train_bce</td><td>███████▇▇▇▅▅▄▆▄▄▄▅▃▄▂▅▄▃▄▅▂▁▃▅▃▂▅▄▂▃▅▅▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_inter_edge_gamma_1</td><td>0.0025</td></tr><tr><td>avg_inter_edge_gamma_2</td><td>0.0</td></tr><tr><td>avg_intra_edge_gamma_1</td><td>0.0025</td></tr><tr><td>avg_intra_edge_gamma_2</td><td>0.0</td></tr><tr><td>epoch</td><td>1249</td></tr><tr><td>mu</td><td>0.00049</td></tr><tr><td>std_inter_edge_gamma_1</td><td>0.04994</td></tr><tr><td>std_inter_edge_gamma_2</td><td>0.0</td></tr><tr><td>std_intra_edge_gamma_1</td><td>0.04994</td></tr><tr><td>std_intra_edge_gamma_2</td><td>0.0</td></tr><tr><td>train_bce</td><td>0.13062</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">icy-glitter-15</strong>: <a href=\"https://wandb.ai/rish-16/GATv3/runs/3itep3wk\" target=\"_blank\">https://wandb.ai/rish-16/GATv3/runs/3itep3wk</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220525_014040-3itep3wk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/rishabh/Desktop/graph-attention-analysis/src/wandb/run-20220525_020008-2qfncqg8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/rish-16/GATv3/runs/2qfncqg8\" target=\"_blank\">golden-wildflower-16</a></strong> to <a href=\"https://wandb.ai/rish-16/GATv3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Mu: 0.0006629824803044508\n",
      "Epoch: 0 | Train BCE: 0.7291097044944763\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [96]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[1;32m     79\u001b[0m     optimiser\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 80\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mgat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXw_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     loss \u001b[38;5;241m=\u001b[39m crit(pred, ground_truth_tensor)\n\u001b[1;32m     82\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/PyG/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [92]\u001b[0m, in \u001b[0;36mGATv3.forward\u001b[0;34m(self, x, edge_idx)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_idx):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 11\u001b[0m         eigen_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meigen\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgat1(x, edge_idx, eigen_x))\n\u001b[1;32m     13\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgat2(x, edge_idx, eigen_x)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/PyG/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [91]\u001b[0m, in \u001b[0;36mEigen.forward\u001b[0;34m(self, edge_idx)\u001b[0m\n\u001b[1;32m     18\u001b[0m     x_j_prime \u001b[38;5;241m=\u001b[39m top_eig[j]\n\u001b[1;32m     19\u001b[0m     new_feat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x_i_prime, x_j_prime], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m     new_edge_features[idx] \u001b[38;5;241m=\u001b[39m new_feat\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_edge_features\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n = 400\n",
    "d = int(np.ceil(n/(np.log(n)**2)))\n",
    "p = 0.5\n",
    "q = 0.1\n",
    "\n",
    "sizes = [int(n/2), int(n/2)]\n",
    "probs = [[p,q], [q,p]]\n",
    "\n",
    "std_ = 0.1\n",
    "mu_up = 20*std_*np.sqrt(np.log(n**2))/(2*np.sqrt(d))\n",
    "mu_lb = 0.01*std_/(2*np.sqrt(d))\n",
    "\n",
    "mus = np.geomspace(mu_lb, mu_up, 30, endpoint=True)\n",
    "ground_truth = np.concatenate((np.zeros(int(n/2)), np.ones(int(n/2))))\n",
    "\n",
    "for mu in mus:\n",
    "    g = nx.stochastic_block_model(sizes, probs)\n",
    "    adjlist = [[v for v in g.neighbors(i)] for i in range(n)]\n",
    "\n",
    "    adj_matrix = [[0 for i in range(n)] for j in range(n)]\n",
    "    for i in range(n):\n",
    "        nbors = g.neighbors(i)\n",
    "        for j in nbors:\n",
    "            adj_matrix[i][j] = 1\n",
    "\n",
    "    edge_idx, _ = tg.utils.dense_to_sparse(torch.from_numpy(np.array(adj_matrix)))\n",
    "\n",
    "    for i in range(len(adjlist)):\n",
    "        adjlist[i].append(i) # self-loops\n",
    "\n",
    "    X = np.zeros((n,d))\n",
    "    X[:int(n/2)] = -mu\n",
    "    X[int(n/2):] = mu\n",
    "    noise = std_*np.random.randn(n,d)\n",
    "    X = X + noise\n",
    "\n",
    "    R = 1\n",
    "    mu_ = mu*np.ones(d)\n",
    "    w = (R/np.linalg.norm(mu_))*mu_\n",
    "    Xw = X@w\n",
    "\n",
    "    wandb.init(project=\"GATv3\", entity=\"rish-16\")\n",
    "\n",
    "    HIDDEN = 16\n",
    "    eigenK = 10\n",
    "    EPOCHS = 1250\n",
    "\n",
    "    gat = GATv3(\n",
    "        indim=1, \n",
    "        eigendim=20, \n",
    "        hidden=HIDDEN, \n",
    "        outdim=1, \n",
    "        k=eigenK\n",
    "    ) # take top 10 eigen vector features\n",
    "    crit = nn.BCEWithLogitsLoss()\n",
    "    optimiser = torch.optim.Adam(gat.parameters())\n",
    "\n",
    "    wandb.config = {\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"n_nodes\": n,\n",
    "        \"eigenK\": eigenK,\n",
    "        \"optimiser\": \"Adam\",\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"n_layers\": 2,\n",
    "        \"std\": std_,\n",
    "        \"p\": p,\n",
    "        \"q\": q,\n",
    "        \"d\": d\n",
    "    }\n",
    "\n",
    "    wandb.watch(gat)\n",
    "\n",
    "    Xw_tensor = torch.from_numpy(Xw).unsqueeze(-1).float()\n",
    "    ground_truth_tensor = torch.from_numpy(ground_truth).unsqueeze(-1).float()\n",
    "    \n",
    "    print (f\"Training with Mu: {mu}\")\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        optimiser.step()\n",
    "        pred = gat(Xw_tensor, edge_idx)\n",
    "        loss = crit(pred, ground_truth_tensor)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_bce\": loss.item()\n",
    "        })    \n",
    "\n",
    "        if epoch % 200 == 0:\n",
    "            print (f\"Epoch: {epoch} | Train BCE: {loss.item()}\")\n",
    "            \n",
    "    print (\"------------------------------------------\\n\\n\")\n",
    "\n",
    "    gamma_matrix1 = get_gammas(Xw_tensor, ground_truth_tensor, gat.gat1, edge_idx)\n",
    "    gamma_matrix2 = get_gammas(Xw_tensor, ground_truth_tensor, gat.gat2, edge_idx)\n",
    "\n",
    "    intra1, inter1 = get_intra_inter_avg_gamma(gamma_matrix1)\n",
    "    intra2, inter2 = get_intra_inter_avg_gamma(gamma_matrix2)\n",
    "\n",
    "    avg_intra_gamma_1 = intra1.mean()\n",
    "    avg_inter_gamma_1 = inter1.mean()\n",
    "\n",
    "    avg_intra_gamma_2 = intra2.mean()\n",
    "    avg_inter_gamma_2 = inter2.mean()\n",
    "    \n",
    "    std_intra_gamma_1 = intra1.std()\n",
    "    std_inter_gamma_1 = inter1.std()\n",
    "\n",
    "    std_intra_gamma_2 = intra2.std()\n",
    "    std_inter_gamma_2 = inter2.std()\n",
    "\n",
    "    wandb.log({\n",
    "        \"mu\": mu,\n",
    "        \"avg_intra_edge_gamma_1\": avg_intra_gamma_1,\n",
    "        \"avg_inter_edge_gamma_1\": avg_inter_gamma_1,\n",
    "        \"avg_intra_edge_gamma_2\": avg_intra_gamma_2,\n",
    "        \"avg_inter_edge_gamma_2\": avg_inter_gamma_2,\n",
    "        \"std_intra_edge_gamma_1\": std_intra_gamma_1,\n",
    "        \"std_inter_edge_gamma_1\": std_inter_gamma_1,\n",
    "        \"std_intra_edge_gamma_2\": std_intra_gamma_2,\n",
    "        \"std_inter_edge_gamma_2\": std_inter_gamma_2,\n",
    "    })\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6066441-40ca-4ffc-8dd0-2cb871a4916b",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "    e &= \\operatorname{EigenDecomp}(I - D^{-1\\frac{1}{2}}AD^{-1\\frac{1}{2}}) \\\\\\\\\n",
    "    \\gamma_{ij} &= \\operatorname{Softmax}(\\operatorname{LeakyReLU}(\\alpha \\cdot \\phi (Wx_i || Wx_j) + \\beta \\cdot \\psi (We_i || We_j))) \\\\\\\\\n",
    "    x^{ l+1}_{i} &= \\sigma(Wx_i + \\Sigma_{j \\in N_j} W \\gamma_{ij} x_j)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1884d562-58e7-4129-9551-772ed636c14b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
